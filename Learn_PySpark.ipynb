{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9e3ef16-df47-483c-9cec-9129041b0d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.0.0\n=== Employees DataFrame ===\n+-----------+-------------+----------+------+----------+\n|employee_id|employee_name|department|salary| hire_date|\n+-----------+-------------+----------+------+----------+\n|          1|   John Smith|     Sales| 50000|2020-01-15|\n|          2|     Jane Doe|     Sales| 55000|2019-03-20|\n|          3| Mike Johnson| Marketing| 60000|2018-06-10|\n|          4| Sarah Wilson|     Sales| 52000|2020-11-05|\n|          5|  David Brown|        IT| 70000|2017-09-12|\n|          6|  Emily Davis| Marketing| 58000|2019-08-22|\n|          7|   Robert Lee|     Sales| 53000|2021-02-28|\n|          8|  Lisa Garcia|        IT| 72000|2016-12-01|\n|          9|   Tom Miller|     Sales| 51000|2022-01-10|\n|         10|    Amy Clark| Marketing| 59000|2018-04-15|\n+-----------+-------------+----------+------+----------+\n\nSchema:\nroot\n |-- employee_id: integer (nullable = true)\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- hire_date: date (nullable = true)\n\n\n=== Sales DataFrame ===\n+-------+-----------+----------+------+------+----------------+\n|sale_id|employee_id| sale_date|amount|region|product_category|\n+-------+-----------+----------+------+------+----------------+\n|      1|          1|2024-01-15|1500.0| North|     Electronics|\n|      2|          2|2024-01-16|2000.0| South|       Furniture|\n|      3|          1|2024-01-17|1200.0| North|     Electronics|\n|      4|          3|2024-01-18|1800.0|  East| Office Supplies|\n|      5|          4|2024-01-19|2200.0|  West|       Furniture|\n|      6|          2|2024-01-20|1900.0| South|     Electronics|\n|      7|          1|2024-01-21|2100.0| North|       Furniture|\n|      8|          5|2024-01-22|1700.0|  East|     Electronics|\n|      9|          4|2024-01-23|2400.0|  West| Office Supplies|\n|     10|          3|2024-01-24|1600.0|  East|       Furniture|\n|     11|          2|2024-02-01|2300.0| South|     Electronics|\n|     12|          1|2024-02-02|1900.0| North| Office Supplies|\n|     13|          4|2024-02-03|2100.0|  West|       Furniture|\n|     14|          3|2024-02-04|1700.0|  East|     Electronics|\n|     15|          2|2024-02-05|2500.0| South|       Furniture|\n+-------+-----------+----------+------+------+----------------+\n\nSchema:\nroot\n |-- sale_id: integer (nullable = true)\n |-- employee_id: integer (nullable = true)\n |-- sale_date: date (nullable = true)\n |-- amount: double (nullable = true)\n |-- region: string (nullable = true)\n |-- product_category: string (nullable = true)\n\n\n==================================================\nSECTION 3: BASIC DATAFRAME OPERATIONS\n==================================================\n\n--- 1. Select Specific Columns and Filter ---\nSales Department Employees (High to Low Salary):\n+-----------+-------------+----------+------+\n|employee_id|employee_name|department|salary|\n+-----------+-------------+----------+------+\n|          2|     Jane Doe|     Sales| 55000|\n|          7|   Robert Lee|     Sales| 53000|\n|          4| Sarah Wilson|     Sales| 52000|\n|          9|   Tom Miller|     Sales| 51000|\n|          1|   John Smith|     Sales| 50000|\n+-----------+-------------+----------+------+\n\n\n--- 2. Adding Derived Columns ---\nEmployees with Bonus and Compensation Details:\n+-------------+----------+------+------------+------------------+----------------+\n|employee_name|department|salary|annual_bonus|total_compensation|years_of_service|\n+-------------+----------+------+------------+------------------+----------------+\n|   John Smith|     Sales| 50000|      5000.0|           55000.0|               5|\n|     Jane Doe|     Sales| 55000|      5500.0|           60500.0|               6|\n| Mike Johnson| Marketing| 60000|      6000.0|           66000.0|               7|\n| Sarah Wilson|     Sales| 52000|      5200.0|           57200.0|               5|\n|  David Brown|        IT| 70000|      7000.0|           77000.0|               8|\n|  Emily Davis| Marketing| 58000|      5800.0|           63800.0|               6|\n|   Robert Lee|     Sales| 53000|      5300.0|           58300.0|               4|\n|  Lisa Garcia|        IT| 72000|      7200.0|           79200.0|               9|\n|   Tom Miller|     Sales| 51000|      5100.0|           56100.0|               3|\n|    Amy Clark| Marketing| 59000|      5900.0|           64900.0|               7|\n+-------------+----------+------+------------+------------------+----------------+\n\n\n--- 3. Department-wise Aggregations ---\nDepartment-wise Statistics:\n+----------+--------------+--------------+----------+----------+-------------------+\n|department|employee_count|average_salary|max_salary|min_salary|total_salary_budget|\n+----------+--------------+--------------+----------+----------+-------------------+\n|        IT|             2|       71000.0|     72000|     70000|             142000|\n| Marketing|             3|       59000.0|     60000|     58000|             177000|\n|     Sales|             5|       52200.0|     55000|     50000|             261000|\n+----------+--------------+--------------+----------+----------+-------------------+\n\n\n--- 4. Joining Employees and Sales Data ---\nEmployees with Sales Data:\n+-----------+-------------+----------+----------+------+------+----------------+\n|employee_id|employee_name|department| sale_date|amount|region|product_category|\n+-----------+-------------+----------+----------+------+------+----------------+\n|          2|     Jane Doe|     Sales|2024-02-05|2500.0| South|       Furniture|\n|          4| Sarah Wilson|     Sales|2024-01-23|2400.0|  West| Office Supplies|\n|          2|     Jane Doe|     Sales|2024-02-01|2300.0| South|     Electronics|\n|          4| Sarah Wilson|     Sales|2024-01-19|2200.0|  West|       Furniture|\n|          1|   John Smith|     Sales|2024-01-21|2100.0| North|       Furniture|\n|          4| Sarah Wilson|     Sales|2024-02-03|2100.0|  West|       Furniture|\n|          2|     Jane Doe|     Sales|2024-01-16|2000.0| South|       Furniture|\n|          2|     Jane Doe|     Sales|2024-01-20|1900.0| South|     Electronics|\n|          1|   John Smith|     Sales|2024-02-02|1900.0| North| Office Supplies|\n|          3| Mike Johnson| Marketing|2024-01-18|1800.0|  East| Office Supplies|\n|          3| Mike Johnson| Marketing|2024-02-04|1700.0|  East|     Electronics|\n|          5|  David Brown|        IT|2024-01-22|1700.0|  East|     Electronics|\n|          3| Mike Johnson| Marketing|2024-01-24|1600.0|  East|       Furniture|\n|          1|   John Smith|     Sales|2024-01-15|1500.0| North|     Electronics|\n|          1|   John Smith|     Sales|2024-01-17|1200.0| North|     Electronics|\n+-----------+-------------+----------+----------+------+------+----------------+\n\n\n==================================================\nSECTION 4: WINDOW FUNCTIONS - RANKING\n==================================================\n\n--- 5. ROW_NUMBER() - Unique Ranking ---\nEmployees with Ranking Functions:\n+-------------+----------+------+----------+-----------+----------+\n|employee_name|department|salary|row_number|salary_rank|dense_rank|\n+-------------+----------+------+----------+-----------+----------+\n|  Lisa Garcia|        IT| 72000|         1|          1|         1|\n|  David Brown|        IT| 70000|         2|          2|         2|\n| Mike Johnson| Marketing| 60000|         1|          1|         1|\n|    Amy Clark| Marketing| 59000|         2|          2|         2|\n|  Emily Davis| Marketing| 58000|         3|          3|         3|\n|     Jane Doe|     Sales| 55000|         1|          1|         1|\n|   Robert Lee|     Sales| 53000|         2|          2|         2|\n| Sarah Wilson|     Sales| 52000|         3|          3|         3|\n|   Tom Miller|     Sales| 51000|         4|          4|         4|\n|   John Smith|     Sales| 50000|         5|          5|         5|\n+-------------+----------+------+----------+-----------+----------+\n\n\n--- 6. NTILE() - Salary Quartiles ---\nEmployees with Salary Quartiles:\n+-------------+----------+------+---------------+\n|employee_name|department|salary|salary_quartile|\n+-------------+----------+------+---------------+\n|  Lisa Garcia|        IT| 72000|              1|\n|  David Brown|        IT| 70000|              2|\n| Mike Johnson| Marketing| 60000|              1|\n|    Amy Clark| Marketing| 59000|              2|\n|  Emily Davis| Marketing| 58000|              3|\n|     Jane Doe|     Sales| 55000|              1|\n|   Robert Lee|     Sales| 53000|              1|\n| Sarah Wilson|     Sales| 52000|              2|\n|   Tom Miller|     Sales| 51000|              3|\n|   John Smith|     Sales| 50000|              4|\n+-------------+----------+------+---------------+\n\n\n==================================================\nSECTION 5: WINDOW FUNCTIONS - ANALYTICAL\n==================================================\n\n--- 7. LAG() and LEAD() - Compare Adjacent Values ---\nSalary Comparision with Previous/Next:\n+-------------+----------+------+---------------+-----------+-------------+--------------+\n|employee_name|department|salary|previous_salary|next_salary|salary_growth|salary_decline|\n+-------------+----------+------+---------------+-----------+-------------+--------------+\n|  David Brown|        IT| 70000|           NULL|      72000|            0|          2000|\n|  Lisa Garcia|        IT| 72000|          70000|       NULL|         2000|             0|\n|  Emily Davis| Marketing| 58000|           NULL|      59000|            0|          1000|\n|    Amy Clark| Marketing| 59000|          58000|      60000|         1000|          1000|\n| Mike Johnson| Marketing| 60000|          59000|       NULL|         1000|             0|\n|   John Smith|     Sales| 50000|           NULL|      51000|            0|          1000|\n|   Tom Miller|     Sales| 51000|          50000|      52000|         1000|          1000|\n| Sarah Wilson|     Sales| 52000|          51000|      53000|         1000|          1000|\n|   Robert Lee|     Sales| 53000|          52000|      55000|         1000|          2000|\n|     Jane Doe|     Sales| 55000|          53000|       NULL|         2000|             0|\n+-------------+----------+------+---------------+-----------+-------------+--------------+\n\n\n--- 8. FIRST_VALUE() and LAST_VALUE() ---\nSalary Extremes within Departments:\n+-------------+----------+------+--------------+--------------+-------------+-----------------+\n|employee_name|department|salary|department_min|department_max|diff_from_min|   percent_of_max|\n+-------------+----------+------+--------------+--------------+-------------+-----------------+\n|  David Brown|        IT| 70000|         70000|         72000|            0|97.22222222222221|\n|  Lisa Garcia|        IT| 72000|         70000|         72000|         2000|            100.0|\n|  Emily Davis| Marketing| 58000|         58000|         60000|            0|96.66666666666667|\n|    Amy Clark| Marketing| 59000|         58000|         60000|         1000|98.33333333333333|\n| Mike Johnson| Marketing| 60000|         58000|         60000|         2000|            100.0|\n|   John Smith|     Sales| 50000|         50000|         55000|            0| 90.9090909090909|\n|   Tom Miller|     Sales| 51000|         50000|         55000|         1000|92.72727272727272|\n| Sarah Wilson|     Sales| 52000|         50000|         55000|         2000|94.54545454545455|\n|   Robert Lee|     Sales| 53000|         50000|         55000|         3000|96.36363636363636|\n|     Jane Doe|     Sales| 55000|         50000|         55000|         5000|            100.0|\n+-------------+----------+------+--------------+--------------+-------------+-----------------+\n\n\n==================================================\nSECTION 6: WINDOW FUNCTIONS - AGGREGATES\n==================================================\n\n--- 9. Running Totals and Moving Averages ---\nSales Analysis with Running Totals:\n+-------------+----------+------+-------------+------------------+------------------+\n|employee_name| sale_date|amount|running_total|      moving_avg_3|  percent_of_total|\n+-------------+----------+------+-------------+------------------+------------------+\n|  David Brown|2024-01-22|1700.0|       1700.0|            1700.0|             100.0|\n|     Jane Doe|2024-01-16|2000.0|       2000.0|            2000.0|22.988505747126435|\n|     Jane Doe|2024-01-20|1900.0|       3900.0|            1950.0|21.839080459770116|\n|     Jane Doe|2024-02-01|2300.0|       6200.0|2066.6666666666665|26.436781609195403|\n|     Jane Doe|2024-02-05|2500.0|       8700.0|2233.3333333333335|28.735632183908045|\n|   John Smith|2024-01-15|1500.0|       1500.0|            1500.0|22.388059701492537|\n|   John Smith|2024-01-17|1200.0|       2700.0|            1350.0| 17.91044776119403|\n|   John Smith|2024-01-21|2100.0|       4800.0|            1600.0|31.343283582089555|\n|   John Smith|2024-02-02|1900.0|       6700.0|1733.3333333333333| 28.35820895522388|\n| Mike Johnson|2024-01-18|1800.0|       1800.0|            1800.0|35.294117647058826|\n| Mike Johnson|2024-01-24|1600.0|       3400.0|            1700.0|31.372549019607842|\n| Mike Johnson|2024-02-04|1700.0|       5100.0|            1700.0| 33.33333333333333|\n| Sarah Wilson|2024-01-19|2200.0|       2200.0|            2200.0| 32.83582089552239|\n| Sarah Wilson|2024-01-23|2400.0|       4600.0|            2300.0| 35.82089552238806|\n| Sarah Wilson|2024-02-03|2100.0|       6700.0|2233.3333333333335|31.343283582089555|\n+-------------+----------+------+-------------+------------------+------------------+\n\n\n==================================================\nSECTION 7: COMPLEX WINDOW SCENARIOS\n==================================================\n\n--- 10. Comprehensive Salary Analysis ---\nComprehensive Salary Analysis:\n+-------------+----------+------+---------+---------------+-------------+-----------------+------------------+\n|employee_name|department|salary|dept_rank|dept_avg_salary|diff_from_avg|   percent_of_max|      salary_ratio|\n+-------------+----------+------+---------+---------------+-------------+-----------------+------------------+\n|  Lisa Garcia|        IT| 72000|        1|        71000.0|       1000.0|            100.0|1.0140845070422535|\n|  David Brown|        IT| 70000|        2|        71000.0|      -1000.0|97.22222222222221|0.9859154929577465|\n| Mike Johnson| Marketing| 60000|        1|        59000.0|       1000.0|            100.0|1.0169491525423728|\n|    Amy Clark| Marketing| 59000|        2|        59000.0|          0.0|98.33333333333333|               1.0|\n|  Emily Davis| Marketing| 58000|        3|        59000.0|      -1000.0|96.66666666666667|0.9830508474576272|\n|     Jane Doe|     Sales| 55000|        1|        52200.0|       2800.0|            100.0| 1.053639846743295|\n|   Robert Lee|     Sales| 53000|        2|        52200.0|        800.0|96.36363636363636|1.0153256704980842|\n| Sarah Wilson|     Sales| 52000|        3|        52200.0|       -200.0|94.54545454545455|0.9961685823754789|\n|   Tom Miller|     Sales| 51000|        4|        52200.0|      -1200.0|92.72727272727272|0.9770114942528736|\n|   John Smith|     Sales| 50000|        5|        52200.0|      -2200.0| 90.9090909090909|0.9578544061302682|\n+-------------+----------+------+---------+---------------+-------------+-----------------+------------------+\n\n\n--- 11. Multi-dimensional Sales Analysis ---\nSales Performance Analysis:\n+-------------+------+----------------+------+-------------+-------------+-------------------+------------------+---------------------+\n|employee_name|region|product_category|amount|regional_rank|category_rank|employee_region_avg|      regional_avg|performance_vs_region|\n+-------------+------+----------------+------+-------------+-------------+-------------------+------------------+---------------------+\n| Mike Johnson|  East| Office Supplies|1800.0|            1|            3|             1700.0|            1700.0|                100.0|\n|  David Brown|  East|     Electronics|1700.0|            2|            3|             1700.0|            1700.0|                  0.0|\n| Mike Johnson|  East|     Electronics|1700.0|            2|            3|             1700.0|            1700.0|                  0.0|\n| Mike Johnson|  East|       Furniture|1600.0|            4|            6|             1700.0|            1700.0|               -100.0|\n|   John Smith| North|       Furniture|2100.0|            1|            3|             1675.0|            1675.0|                425.0|\n|   John Smith| North| Office Supplies|1900.0|            2|            2|             1675.0|            1675.0|                225.0|\n|   John Smith| North|     Electronics|1500.0|            3|            5|             1675.0|            1675.0|               -175.0|\n|   John Smith| North|     Electronics|1200.0|            4|            6|             1675.0|            1675.0|               -475.0|\n|     Jane Doe| South|       Furniture|2500.0|            1|            1|             2175.0|            2175.0|                325.0|\n|     Jane Doe| South|     Electronics|2300.0|            2|            1|             2175.0|            2175.0|                125.0|\n|     Jane Doe| South|       Furniture|2000.0|            3|            5|             2175.0|            2175.0|               -175.0|\n|     Jane Doe| South|     Electronics|1900.0|            4|            2|             2175.0|            2175.0|               -275.0|\n| Sarah Wilson|  West| Office Supplies|2400.0|            1|            1| 2233.3333333333335|2233.3333333333335|   166.66666666666652|\n| Sarah Wilson|  West|       Furniture|2200.0|            2|            2| 2233.3333333333335|2233.3333333333335|  -33.333333333333485|\n| Sarah Wilson|  West|       Furniture|2100.0|            3|            3| 2233.3333333333335|2233.3333333333335|  -133.33333333333348|\n+-------------+------+----------------+------+-------------+-------------+-------------------+------------------+---------------------+\n\n\n==================================================\nSECTION 8: PRACTICAL BUSINESS SCENARIOS\n==================================================\n\n--- Scenario 1: Performance Bonus Calculation ---\nEmployee Bonus Calculation:\n+-------------+----------+------+---------+------------------+------------+\n|employee_name|department|salary|dept_rank|        bonus_tier|bonus_amount|\n+-------------+----------+------+---------+------------------+------------+\n|  Lisa Garcia|        IT| 72000|        1|   Gold Bonus: 20%|     14400.0|\n|  David Brown|        IT| 70000|        2| Silver Bonus: 15%|     10500.0|\n| Mike Johnson| Marketing| 60000|        1|   Gold Bonus: 20%|     12000.0|\n|    Amy Clark| Marketing| 59000|        2| Silver Bonus: 15%|      8850.0|\n|  Emily Davis| Marketing| 58000|        3| Bronze Bonus: 10%|      5800.0|\n|     Jane Doe|     Sales| 55000|        1|   Gold Bonus: 20%|     11000.0|\n|   Robert Lee|     Sales| 53000|        2| Silver Bonus: 15%|      7950.0|\n| Sarah Wilson|     Sales| 52000|        3| Bronze Bonus: 10%|      5200.0|\n|   Tom Miller|     Sales| 51000|        4|Standard Bonus: 5%|      2550.0|\n|   John Smith|     Sales| 50000|        5|Standard Bonus: 5%|      2500.0|\n+-------------+----------+------+---------+------------------+------------+\n\n\n--- Scenario 2: Sales Trend Analysis ---\nSales Trend Analysis:\n+-------------+----------+------+-------------+-----------+-----------------+\n|employee_name| sale_date|amount|previous_sale|sales_trend|growth_percentage|\n+-------------+----------+------+-------------+-----------+-----------------+\n|  David Brown|2024-01-22|1700.0|         NULL| First Sale|              0.0|\n|     Jane Doe|2024-01-16|2000.0|         NULL| First Sale|              0.0|\n|     Jane Doe|2024-01-20|1900.0|       2000.0|    Decline|             -5.0|\n|     Jane Doe|2024-02-01|2300.0|       1900.0|     Growth|            21.05|\n|     Jane Doe|2024-02-05|2500.0|       2300.0|     Growth|              8.7|\n|   John Smith|2024-01-15|1500.0|         NULL| First Sale|              0.0|\n|   John Smith|2024-01-17|1200.0|       1500.0|    Decline|            -20.0|\n|   John Smith|2024-01-21|2100.0|       1200.0|     Growth|             75.0|\n|   John Smith|2024-02-02|1900.0|       2100.0|    Decline|            -9.52|\n| Mike Johnson|2024-01-18|1800.0|         NULL| First Sale|              0.0|\n| Mike Johnson|2024-01-24|1600.0|       1800.0|    Decline|           -11.11|\n| Mike Johnson|2024-02-04|1700.0|       1600.0|     Growth|             6.25|\n| Sarah Wilson|2024-01-19|2200.0|         NULL| First Sale|              0.0|\n| Sarah Wilson|2024-01-23|2400.0|       2200.0|     Growth|             9.09|\n| Sarah Wilson|2024-02-03|2100.0|       2400.0|    Decline|            -12.5|\n+-------------+----------+------+-------------+-----------+-----------------+\n\n\n==================================================\nSECTION 9: EXPORTING RESULTS\n==================================================\n\n--- Saving Results to Delta Tables ---\n"
     ]
    }
   ],
   "source": [
    "# DATAFRAME API COMPREHENSIVE PROJECT\n",
    "# Dataset: Employee Sales Performance Data\n",
    "# Using PySpark DataFrame API\n",
    "# =============================================\n",
    "\n",
    "# SECTION 1: SETUP AND DATA CREATION\n",
    "# =============================================\n",
    "\n",
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark Session\n",
    "# Note: In Databricks, spark session is already created\n",
    "spark = SparkSession.builder.appName(\"DataFrameAPIProject\").getOrCreate()\n",
    "\n",
    "# Display Spark version and configuration\n",
    "print(\"Spark Version:\", spark.version)\n",
    "\n",
    "# Get Data from Sample Data\n",
    "# =============================================\n",
    "# Create Employees DataFrame\n",
    "employees_data = [\n",
    "    (1, \"John Smith\", \"Sales\", 50000, \"2020-01-15\"),\n",
    "    (2, \"Jane Doe\", \"Sales\", 55000, \"2019-03-20\"),\n",
    "    (3, \"Mike Johnson\", \"Marketing\", 60000, \"2018-06-10\"),\n",
    "    (4, \"Sarah Wilson\", \"Sales\", 52000, \"2020-11-05\"),\n",
    "    (5, \"David Brown\", \"IT\", 70000, \"2017-09-12\"),\n",
    "    (6, \"Emily Davis\", \"Marketing\", 58000, \"2019-08-22\"),\n",
    "    (7, \"Robert Lee\", \"Sales\", 53000, \"2021-02-28\"),\n",
    "    (8, \"Lisa Garcia\", \"IT\", 72000, \"2016-12-01\"),\n",
    "    (9, \"Tom Miller\", \"Sales\", 51000, \"2022-01-10\"),\n",
    "    (10, \"Amy Clark\", \"Marketing\", 59000, \"2018-04-15\")\n",
    "]\n",
    "\n",
    "employees_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"employee_name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"hire_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "employees_df = spark.createDataFrame(employees_data, employees_schema)\n",
    "\n",
    "# Convert hire_date string to DateType\n",
    "employees_df = employees_df.withColumn(\"hire_date\", to_date(col(\"hire_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "print(\"=== Employees DataFrame ===\")\n",
    "employees_df.show()\n",
    "print(\"Schema:\")\n",
    "employees_df.printSchema()\n",
    "\n",
    "# Create Sales DataFrame\n",
    "sales_data = [\n",
    "    (1, 1, \"2024-01-15\", 1500.00, \"North\", \"Electronics\"),\n",
    "    (2, 2, \"2024-01-16\", 2000.00, \"South\", \"Furniture\"),\n",
    "    (3, 1, \"2024-01-17\", 1200.00, \"North\", \"Electronics\"),\n",
    "    (4, 3, \"2024-01-18\", 1800.00, \"East\", \"Office Supplies\"),\n",
    "    (5, 4, \"2024-01-19\", 2200.00, \"West\", \"Furniture\"),\n",
    "    (6, 2, \"2024-01-20\", 1900.00, \"South\", \"Electronics\"),\n",
    "    (7, 1, \"2024-01-21\", 2100.00, \"North\", \"Furniture\"),\n",
    "    (8, 5, \"2024-01-22\", 1700.00, \"East\", \"Electronics\"),\n",
    "    (9, 4, \"2024-01-23\", 2400.00, \"West\", \"Office Supplies\"),\n",
    "    (10, 3, \"2024-01-24\", 1600.00, \"East\", \"Furniture\"),\n",
    "    (11, 2, \"2024-02-01\", 2300.00, \"South\", \"Electronics\"),\n",
    "    (12, 1, \"2024-02-02\", 1900.00, \"North\", \"Office Supplies\"),\n",
    "    (13, 4, \"2024-02-03\", 2100.00, \"West\", \"Furniture\"),\n",
    "    (14, 3, \"2024-02-04\", 1700.00, \"East\", \"Electronics\"),\n",
    "    (15, 2, \"2024-02-05\", 2500.00, \"South\", \"Furniture\")\n",
    "]\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField(\"sale_id\", IntegerType(), True),\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"sale_date\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"product_category\", StringType(), True)\n",
    "])\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "\n",
    "# Convert sale_date string to DateType\n",
    "sales_df = sales_df.withColumn(\"sale_date\", to_date(col(\"sale_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "print(\"\\n=== Sales DataFrame ===\")\n",
    "sales_df.show()\n",
    "print(\"Schema:\")\n",
    "sales_df.printSchema()\n",
    "\n",
    "# SECTION 3: BASIC DATAFRAME OPERATIONS\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 3: BASIC DATAFRAME OPERATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Operation 1: Select and Filter\n",
    "# Purpose: Demonstrate column selection and data filtering\n",
    "print(\"\\n--- 1. Select Specific Columns and Filter ---\")\n",
    "filtered_employees = employees_df.select(\n",
    "    \"employee_id\", \n",
    "    \"employee_name\", \n",
    "    \"department\",\n",
    "    \"salary\"\n",
    "    ).filter(\n",
    "        col(\"department\") == \"Sales\"\n",
    "    ).orderBy(\n",
    "        col(\"salary\").desc()\n",
    "    )\n",
    "\n",
    "print(\"Sales Department Employees (High to Low Salary):\")\n",
    "filtered_employees.show()\n",
    "\n",
    "# Operation 2: Adding New Columns\n",
    "# Purpose: Show how to create derived columns\n",
    "print(\"\\n--- 2. Adding Derived Columns ---\")\n",
    "employees_with_bonus = employees_df.withColumn(\n",
    "    \"annual_bonus\", col(\"salary\") * 0.1  # 10% bonus\n",
    ").withColumn(\n",
    "    \"total_compensation\", col(\"salary\") + col(\"annual_bonus\")\n",
    ").withColumn(\n",
    "    \"years_of_service\", year(current_date()) - year(col(\"hire_date\"))\n",
    ")\n",
    "\n",
    "print(\"Employees with Bonus and Compensation Details:\")\n",
    "employees_with_bonus.select(\n",
    "    \"employee_name\",\n",
    "    \"department\",\n",
    "    \"salary\",\n",
    "    \"annual_bonus\",\n",
    "    \"total_compensation\",\n",
    "    \"years_of_service\"\n",
    ").show()\n",
    "\n",
    "# Operation 3: GroupBy and Aggregations\n",
    "# Purpose: Demonstrate data aggregation\n",
    "print(\"\\n--- 3. Department-wise Aggregations ---\")\n",
    "department_stats = employees_df.groupBy(\"department\").agg(\n",
    "    count(\"employee_id\").alias(\"employee_count\"),\n",
    "    avg(\"salary\").alias(\"average_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\"),\n",
    "    min(\"salary\").alias(\"min_salary\"),\n",
    "    sum(\"salary\").alias(\"total_salary_budget\")\n",
    ").orderBy(col(\"average_salary\").desc())\n",
    "\n",
    "print(\"Department-wise Statistics:\")\n",
    "department_stats.show()\n",
    "\n",
    "# Operation 4: Joining DataFrames\n",
    "# Purpose: Combine data from multiple DataFrames\n",
    "print(\"\\n--- 4. Joining Employees and Sales Data ---\")\n",
    "employee_sales = employees_df.alias(\"emp\").join(\n",
    "    sales_df.alias(\"sales\"),\n",
    "    col(\"emp.employee_id\") == col(\"sales.employee_id\"), \n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"emp.employee_id\"),\n",
    "    col(\"emp.employee_name\"),\n",
    "    col(\"emp.department\"),\n",
    "    col(\"sales.sale_date\"),\n",
    "    col(\"sales.amount\"),\n",
    "    col(\"sales.region\"),\n",
    "    col(\"sales.product_category\")\n",
    ").orderBy(col(\"amount\").desc())\n",
    "\n",
    "print(\"Employees with Sales Data:\")\n",
    "employee_sales.show()\n",
    "\n",
    "# SECTION 4: WINDOW FUNCTIONS - RANKING\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 4: WINDOW FUNCTIONS - RANKING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Window Specification: Define how to partition and order data\n",
    "department_window = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Operation 5: ROW_NUMBER()\n",
    "# Purpose: Assign unique sequential numbers within each department\n",
    "print(\"\\n--- 5. ROW_NUMBER() - Unique Ranking ---\")\n",
    "employees_ranked = employees_df.withColumn(\n",
    "    \"row_number\", row_number().over(department_window)\n",
    ").withColumn(\n",
    "    \"salary_rank\", rank().over(department_window)\n",
    ").withColumn(\n",
    "    \"dense_rank\", dense_rank().over(department_window)\n",
    ")\n",
    "\n",
    "print(\"Employees with Ranking Functions:\")\n",
    "employees_ranked.select(\n",
    "    \"employee_name\", \"department\", \"salary\", \n",
    "    \"row_number\",\"salary_rank\", \"dense_rank\"\n",
    ").orderBy(\"department\", col(\"salary\").desc()).show()\n",
    "\n",
    "\n",
    "# Operation 6: NTILE - Quartiles\n",
    "# Purpose: Divide employees into salary quartiles within departments\n",
    "print(\"\\n--- 6. NTILE() - Salary Quartiles ---\")\n",
    "employees_with_quartiles = employees_df.withColumn(\n",
    "    \"salary_quartile\", ntile(4).over(department_window)\n",
    ")\n",
    "\n",
    "print(\"Employees with Salary Quartiles:\")\n",
    "employees_with_quartiles.select(\n",
    "    \"employee_name\", \"department\", \"salary\", \"salary_quartile\"\n",
    ").orderBy(\"department\", \"salary_quartile\").show()\n",
    "\n",
    "# SECTION 5: WINDOW FUNCTIONS - ANALYTICAL\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 5: WINDOW FUNCTIONS - ANALYTICAL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Window for analytical functions (needs full window frame)\n",
    "department_window_analytical = Window.partitionBy(\"department\").orderBy(\"salary\").rowsBetween(\n",
    "    Window.unboundedPreceding, Window.unboundedFollowing\n",
    ")\n",
    "\n",
    "# Operation 7: LAG and LEAD\n",
    "# Purpose: Compare with previous and next values\n",
    "print(\"\\n--- 7. LAG() and LEAD() - Compare Adjacent Values ---\")\n",
    "salary_comparision = employees_df.withColumn(\n",
    "    \"previous_salary\", lag(\"salary\", 1).over(Window.partitionBy(\"department\").orderBy(\"salary\"))\n",
    ").withColumn(\n",
    "    \"next_salary\", lead(\"salary\", 1).over(Window.partitionBy(\"department\").orderBy(\"salary\"))\n",
    ").withColumn(\n",
    "    \"salary_growth\",\n",
    "    when(col(\"previous_salary\").isNull(), 0)\n",
    "    .otherwise(col(\"salary\") - col(\"previous_salary\"))\n",
    ").withColumn(\n",
    "    \"salary_decline\",\n",
    "    when(col(\"next_salary\").isNull(), 0)\n",
    "    .otherwise(col(\"next_salary\") - col(\"salary\"))\n",
    ")\n",
    "\n",
    "print(\"Salary Comparision with Previous/Next:\")\n",
    "salary_comparision.select(\n",
    "    \"employee_name\", \"department\", \"salary\", \"previous_salary\", \"next_salary\", \"salary_growth\", \"salary_decline\"\n",
    ").show()\n",
    "\n",
    "# Operation 8: FIRST_VALUE and LAST_VALUE\n",
    "# Purpose: Get extreme values in window\n",
    "print(\"\\n--- 8. FIRST_VALUE() and LAST_VALUE() ---\")\n",
    "salary_extremes = employees_df.withColumn(\n",
    "    \"department_min\", first(\"salary\").over(department_window_analytical)\n",
    ").withColumn(\n",
    "    \"department_max\", last(\"salary\").over(department_window_analytical)\n",
    ").withColumn(\n",
    "    \"diff_from_min\", col(\"salary\") - col(\"department_min\")\n",
    ").withColumn(\n",
    "    \"percent_of_max\", (col(\"salary\") / col(\"department_max\")) * 100\n",
    ")\n",
    "\n",
    "print(\"Salary Extremes within Departments:\")\n",
    "salary_extremes.select(\n",
    "    \"employee_name\", \"department\", \"salary\", \"department_min\", \"department_max\", \"diff_from_min\", \"percent_of_max\"\n",
    ").show()\n",
    "\n",
    "# SECTION 6: WINDOW FUNCTIONS - AGGREGATES\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 6: WINDOW FUNCTIONS - AGGREGATES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Operation 9: Running Totals and Moving Averages\n",
    "# Purpose: Calculate cumulative and moving aggregates\n",
    "print(\"\\n--- 9. Running Totals and Moving Averages ---\")\n",
    "\n",
    "# Define window for running calculations\n",
    "sales_window = Window.partitionBy(\"employee_id\").orderBy(\"sale_date\").rowsBetween(\n",
    "    Window.unboundedPreceding, Window.currentRow\n",
    ")\n",
    "\n",
    "# Define window for moving average (3 rows)\n",
    "moving_avg_window = Window.partitionBy(\"employee_id\").orderBy(\"sale_date\").rowsBetween(\n",
    "    -2, Window.currentRow\n",
    ")\n",
    "\n",
    "sales_analysis = employee_sales.withColumn(\n",
    "    \"running_total\", sum(\"amount\").over(sales_window)\n",
    ").withColumn(\n",
    "    \"moving_avg_3\", avg(\"amount\").over(moving_avg_window)\n",
    ").withColumn(\n",
    "    \"total_employee_sales\", sum(\"amount\").over(Window.partitionBy(\"employee_name\"))\n",
    ").withColumn(\n",
    "    \"percent_of_total\", (col(\"amount\") / col(\"total_employee_sales\")) * 100\n",
    ")\n",
    "\n",
    "print(\"Sales Analysis with Running Totals:\")\n",
    "sales_analysis.select(\n",
    "    \"employee_name\", \"sale_date\", \"amount\",\n",
    "    \"running_total\", \"moving_avg_3\",\n",
    "    \"percent_of_total\"\n",
    ").orderBy(\"employee_name\", \"sale_date\").show()\n",
    "\n",
    "\n",
    "# SECTION 7: COMPLEX WINDOW SCENARIOS\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 7: COMPLEX WINDOW SCENARIOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Operation 10: Multiple Window Functions Combined\n",
    "# Purpose: Comprehensive analysis using multiple windows\n",
    "print(\"\\n--- 10. Comprehensive Salary Analysis ---\")\n",
    "\n",
    "# Define multiple window specifications\n",
    "dept_window_rank = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "dept_window_agg = Window.partitionBy(\"department\")\n",
    "\n",
    "comprehensive_analysis = employees_df.withColumn(\n",
    "    \"dept_rank\", rank().over(dept_window_rank)\n",
    ").withColumn(\n",
    "    \"dept_avg_salary\", avg(\"salary\").over(dept_window_agg)\n",
    ").withColumn(\n",
    "    \"dept_max_salary\", max(\"salary\").over(dept_window_agg)\n",
    ").withColumn(\n",
    "    \"dept_min_salary\", min(\"salary\").over(dept_window_agg)\n",
    ").withColumn(\n",
    "    \"diff_from_avg\", col(\"salary\") - col(\"dept_avg_salary\")\n",
    ").withColumn(\n",
    "    \"percent_of_max\", (col(\"salary\") / col(\"dept_max_salary\")) * 100\n",
    ").withColumn(\n",
    "    \"salary_ratio\", col(\"salary\") / col(\"dept_avg_salary\")\n",
    ")\n",
    "\n",
    "print(\"Comprehensive Salary Analysis:\")\n",
    "comprehensive_analysis.select(\n",
    "    \"employee_name\", \"department\", \"salary\", \"dept_rank\",\n",
    "    \"dept_avg_salary\", \"diff_from_avg\", \"percent_of_max\", \"salary_ratio\"\n",
    ").orderBy(\"department\", \"dept_rank\").show()\n",
    "\n",
    "# Operation 11: Sales Performance with Multiple Dimensions\n",
    "print(\"\\n--- 11. Multi-dimensional Sales Analysis ---\")\n",
    "\n",
    "# Define multiple windows for different dimensions\n",
    "region_window = Window.partitionBy(\"region\").orderBy(col(\"amount\").desc())\n",
    "category_window = Window.partitionBy(\"product_category\").orderBy(col(\"amount\").desc())\n",
    "employee_region_window = Window.partitionBy(\"employee_name\", \"region\")\n",
    "\n",
    "sales_performance = employee_sales.withColumn(\n",
    "    \"regional_rank\", rank().over(region_window)\n",
    ").withColumn(\n",
    "    \"category_rank\", rank().over(category_window)\n",
    ").withColumn(\n",
    "    \"employee_region_avg\", avg(\"amount\").over(employee_region_window)\n",
    ").withColumn(\n",
    "    \"regional_avg\", avg(\"amount\").over(Window.partitionBy(\"region\"))\n",
    ").withColumn(\n",
    "    \"performance_vs_region\", col(\"amount\") - col(\"regional_avg\")\n",
    ")\n",
    "\n",
    "print(\"Sales Performance Analysis:\")\n",
    "sales_performance.select(\n",
    "    \"employee_name\", \"region\", \"product_category\", \"amount\",\n",
    "    \"regional_rank\", \"category_rank\", \n",
    "    \"employee_region_avg\", \"regional_avg\", \"performance_vs_region\"\n",
    ").orderBy(\"region\", \"regional_rank\").show()\n",
    "\n",
    "# SECTION 8: PRACTICAL BUSINESS SCENARIOS\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 8: PRACTICAL BUSINESS SCENARIOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Scenario 1: Employee Performance Bonus Calculation\n",
    "print(\"\\n--- Scenario 1: Performance Bonus Calculation ---\")\n",
    "\n",
    "bonus_calculation = comprehensive_analysis.withColumn(\n",
    "    \"bonus_tier\",\n",
    "    when(col(\"dept_rank\") == 1, \"Gold Bonus: 20%\")\n",
    "    .when(col(\"dept_rank\") == 2, \"Silver Bonus: 15%\")\n",
    "    .when(col(\"dept_rank\") == 3, \"Bronze Bonus: 10%\")\n",
    "    .otherwise(\"Standard Bonus: 5%\")\n",
    ").withColumn(\n",
    "    \"bonus_amount\",\n",
    "    when(col(\"dept_rank\") == 1, col(\"salary\") * 0.20)\n",
    "    .when(col(\"dept_rank\") == 2, col(\"salary\") * 0.15)\n",
    "    .when(col(\"dept_rank\") == 3, col(\"salary\") * 0.10)\n",
    "    .otherwise(col(\"salary\") * 0.05)\n",
    ")\n",
    "\n",
    "print(\"Employee Bonus Calculation:\")\n",
    "bonus_calculation.select(\n",
    "    \"employee_name\", \"department\", \"salary\", \"dept_rank\",\n",
    "    \"bonus_tier\", \"bonus_amount\"\n",
    ").orderBy(\"department\", \"dept_rank\").show()\n",
    "\n",
    "# Scenario 2: Sales Trend Analysis\n",
    "print(\"\\n--- Scenario 2: Sales Trend Analysis ---\")\n",
    "\n",
    "sales_trend_window = Window.partitionBy(\"employee_name\").orderBy(\"sale_date\")\n",
    "\n",
    "sales_trend_analysis = employee_sales.withColumn(\n",
    "    \"previous_sale\", lag(\"amount\", 1).over(sales_trend_window)\n",
    ").withColumn(\n",
    "    \"sales_trend\",\n",
    "    when(lag(\"amount\", 1).over(sales_trend_window).isNull(), \"First Sale\")\n",
    "    .when(col(\"amount\") > lag(\"amount\", 1).over(sales_trend_window), \"Growth\")\n",
    "    .when(col(\"amount\") < lag(\"amount\", 1).over(sales_trend_window), \"Decline\")\n",
    "    .otherwise(\"Stable\")\n",
    ").withColumn(\n",
    "    \"growth_percentage\",\n",
    "    when(lag(\"amount\", 1).over(sales_trend_window).isNull(), 0)\n",
    "    .otherwise(((col(\"amount\") - lag(\"amount\", 1).over(sales_trend_window)) / \n",
    "                lag(\"amount\", 1).over(sales_trend_window)) * 100)\n",
    ").filter(\n",
    "    col(\"sales_trend\").isNotNull()  # Remove rows where trend cannot be calculated\n",
    ")\n",
    "\n",
    "print(\"Sales Trend Analysis:\")\n",
    "sales_trend_analysis.select(\n",
    "    \"employee_name\", \"sale_date\", \"amount\", \"previous_sale\",\n",
    "    \"sales_trend\", round(\"growth_percentage\", 2).alias(\"growth_percentage\")\n",
    ").orderBy(\"employee_name\", \"sale_date\").show()\n",
    "\n",
    "# SECTION 9: DATAFRAME WRITING AND EXPORT\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 9: EXPORTING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save results to Delta tables (Databricks native format)\n",
    "print(\"\\n--- Saving Results to Delta Tables ---\")\n",
    "\n",
    "# Save employees with rankings\n",
    "employees_ranked.write.mode(\"overwrite\").saveAsTable(\"employee_rankings\")\n",
    "\n",
    "# Save sales analysis\n",
    "sales_analysis.write.mode(\"overwrite\").saveAsTable(\"sales_analysis\")\n",
    "\n",
    "# Save bonus calculation\n",
    "bonus_calculation.write.mode(\"overwrite\").saveAsTable(\"employee_bonuses\")\n",
    "\n",
    "print(\"Results saved to Delta tables:\")\n",
    "print(\"- employee_rankings\")\n",
    "print(\"- sales_analysis\") \n",
    "print(\"- employee_bonuses\")\n",
    "\n",
    "# Display saved tables\n",
    "print(\"\\n--- Displaying Saved Tables ---\")\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "# SECTION 10: PERFORMANCE OPTIMIZATION TIPS\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 10: PERFORMANCE OPTIMIZATION TIPS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\"\"\n",
    "DataFrame API Performance Best Practices:\n",
    "\n",
    "1. **Use Column Operations**: Always prefer column operations over UDFs when possible\n",
    "2. **Partition Wisely**: Window functions perform better with appropriate partitioning\n",
    "3. **Avoid Too Many Partitions**: Too many small partitions can degrade performance\n",
    "4. **Use Built-in Functions**: Spark's built-in functions are optimized\n",
    "5. **Cache Frequently Used DataFrames**: Use df.cache() for DataFrames used multiple times\n",
    "6. **Select Only Needed Columns**: Reduce data movement by selecting necessary columns\n",
    "\n",
    "Example of caching:\n",
    "employees_df.cache()  # Cache for repeated use\n",
    "\"\"\")\n",
    "\n",
    "# Cache example\n",
    "employees_df.cache()\n",
    "print(f\"Employees DataFrame cached: {employees_df.is_cached}\")\n",
    "\n",
    "# Clean up cache (optional)\n",
    "# employees_df.unpersist()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*50)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Learn_PySpark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}