  "source": [
    "# DATAFRAME API COMPREHENSIVE PROJECT\n",
    "# Dataset: Employee Sales Performance Data\n",
    "# Using PySpark DataFrame API\n",
    "# =============================================\n",
    "\n",
    "# SECTION 1: SETUP AND DATA CREATION\n",
    "# =============================================\n",
    "\n",
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark Session\n",
    "# Note: In Databricks, spark session is already created\n",
    "spark = SparkSession.builder.appName(\"DataFrameAPIProject\").getOrCreate()\n",
    "\n",
    "# Display Spark version and configuration\n",
    "print(\"Spark Version:\", spark.version)\n",
    "\n",
    "# Get Data from Sample Data\n",
    "# =============================================\n",
    "# Create Employees DataFrame\n",
    "employees_data = [\n",
    "    (1, \"John Smith\", \"Sales\", 50000, \"2020-01-15\"),\n",
    "    (2, \"Jane Doe\", \"Sales\", 55000, \"2019-03-20\"),\n",
    "    (3, \"Mike Johnson\", \"Marketing\", 60000, \"2018-06-10\"),\n",
    "    (4, \"Sarah Wilson\", \"Sales\", 52000, \"2020-11-05\"),\n",
    "    (5, \"David Brown\", \"IT\", 70000, \"2017-09-12\"),\n",
    "    (6, \"Emily Davis\", \"Marketing\", 58000, \"2019-08-22\"),\n",
    "    (7, \"Robert Lee\", \"Sales\", 53000, \"2021-02-28\"),\n",
    "    (8, \"Lisa Garcia\", \"IT\", 72000, \"2016-12-01\"),\n",
    "    (9, \"Tom Miller\", \"Sales\", 51000, \"2022-01-10\"),\n",
    "    (10, \"Amy Clark\", \"Marketing\", 59000, \"2018-04-15\")\n",
    "]\n",
    "\n",
    "employees_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"employee_name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"hire_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "employees_df = spark.createDataFrame(employees_data, employees_schema)\n",
    "\n",
    "# Convert hire_date string to DateType\n",
    "employees_df = employees_df.withColumn(\"hire_date\", to_date(col(\"hire_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "print(\"=== Employees DataFrame ===\")\n",
    "employees_df.show()\n",
    "print(\"Schema:\")\n",
    "employees_df.printSchema()\n",
    "\n",
    "# Create Sales DataFrame\n",
    "sales_data = [\n",
    "    (1, 1, \"2024-01-15\", 1500.00, \"North\", \"Electronics\"),\n",
    "    (2, 2, \"2024-01-16\", 2000.00, \"South\", \"Furniture\"),\n",
    "    (3, 1, \"2024-01-17\", 1200.00, \"North\", \"Electronics\"),\n",
    "    (4, 3, \"2024-01-18\", 1800.00, \"East\", \"Office Supplies\"),\n",
    "    (5, 4, \"2024-01-19\", 2200.00, \"West\", \"Furniture\"),\n",
    "    (6, 2, \"2024-01-20\", 1900.00, \"South\", \"Electronics\"),\n",
    "    (7, 1, \"2024-01-21\", 2100.00, \"North\", \"Furniture\"),\n",
    "    (8, 5, \"2024-01-22\", 1700.00, \"East\", \"Electronics\"),\n",
    "    (9, 4, \"2024-01-23\", 2400.00, \"West\", \"Office Supplies\"),\n",
    "    (10, 3, \"2024-01-24\", 1600.00, \"East\", \"Furniture\"),\n",
    "    (11, 2, \"2024-02-01\", 2300.00, \"South\", \"Electronics\"),\n",
    "    (12, 1, \"2024-02-02\", 1900.00, \"North\", \"Office Supplies\"),\n",
    "    (13, 4, \"2024-02-03\", 2100.00, \"West\", \"Furniture\"),\n",
    "    (14, 3, \"2024-02-04\", 1700.00, \"East\", \"Electronics\"),\n",
    "    (15, 2, \"2024-02-05\", 2500.00, \"South\", \"Furniture\")\n",
    "]\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField(\"sale_id\", IntegerType(), True),\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"sale_date\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"product_category\", StringType(), True)\n",
    "])\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "\n",
    "# Convert sale_date string to DateType\n",
    "sales_df = sales_df.withColumn(\"sale_date\", to_date(col(\"sale_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "print(\"\\n=== Sales DataFrame ===\")\n",
    "sales_df.show()\n",
    "print(\"Schema:\")\n",
    "sales_df.printSchema()\n",
    "\n",
    "# SECTION 3: BASIC DATAFRAME OPERATIONS\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 3: BASIC DATAFRAME OPERATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Operation 1: Select and Filter\n",
    "# Purpose: Demonstrate column selection and data filtering\n",
    "print(\"\\n--- 1. Select Specific Columns and Filter ---\")\n",
    "filtered_employees = employees_df.select(\n",
    "    \"employee_id\", \n",
    "    \"employee_name\", \n",
    "    \"department\",\n",
    "    \"salary\"\n",
    "    ).filter(\n",
    "        col(\"department\") == \"Sales\"\n",
    "    ).orderBy(\n",
    "        col(\"salary\").desc()\n",
    "    )\n",
    "\n",
    "print(\"Sales Department Employees (High to Low Salary):\")\n",
    "filtered_employees.show()\n",
    "\n",
    "# Operation 2: Adding New Columns\n",
    "# Purpose: Show how to create derived columns\n",
    "print(\"\\n--- 2. Adding Derived Columns ---\")\n",
    "employees_with_bonus = employees_df.withColumn(\n",
    "    \"annual_bonus\", col(\"salary\") * 0.1  # 10% bonus\n",
    ").withColumn(\n",
    "    \"total_compensation\", col(\"salary\") + col(\"annual_bonus\")\n",
    ").withColumn(\n",
    "    \"years_of_service\", year(current_date()) - year(col(\"hire_date\"))\n",
    ")\n",
    "\n",
    "print(\"Employees with Bonus and Compensation Details:\")\n",
    "employees_with_bonus.select(\n",
    "    \"employee_name\",\n",
    "    \"department\",\n",
    "    \"salary\",\n",
    "    \"annual_bonus\",\n",
    "    \"total_compensation\",\n",
    "    \"years_of_service\"\n",
    ").show()\n",
    "\n",
    "# Operation 3: GroupBy and Aggregations\n",
    "# Purpose: Demonstrate data aggregation\n",
    "print(\"\\n--- 3. Department-wise Aggregations ---\")\n",
    "department_stats = employees_df.groupBy(\"department\").agg(\n",
    "    count(\"employee_id\").alias(\"employee_count\"),\n",
    "    avg(\"salary\").alias(\"average_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\"),\n",
    "    min(\"salary\").alias(\"min_salary\"),\n",
    "    sum(\"salary\").alias(\"total_salary_budget\")\n",
    ").orderBy(col(\"average_salary\").desc())\n",
    "\n",
    "print(\"Department-wise Statistics:\")\n",
    "department_stats.show()\n",
    "\n",
    "# Operation 4: Joining DataFrames\n",
    "# Purpose: Combine data from multiple DataFrames\n",
    "print(\"\\n--- 4. Joining Employees and Sales Data ---\")\n",
    "employee_sales = employees_df.alias(\"emp\").join(\n",
    "    sales_df.alias(\"sales\"),\n",
    "    col(\"emp.employee_id\") == col(\"sales.employee_id\"), \n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"emp.employee_id\"),\n",
    "    col(\"emp.employee_name\"),\n",
    "    col(\"emp.department\"),\n",
    "    col(\"sales.sale_date\"),\n",
    "    col(\"sales.amount\"),\n",
    "    col(\"sales.region\"),\n",
    "    col(\"sales.product_category\")\n",
    ").orderBy(col(\"amount\").desc())\n",
    "\n",
    "print(\"Employees with Sales Data:\")\n",
    "employee_sales.show()\n",
    "\n",
    "# SECTION 4: WINDOW FUNCTIONS - RANKING\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 4: WINDOW FUNCTIONS - RANKING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Window Specification: Define how to partition and order data\n",
    "department_window = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Operation 5: ROW_NUMBER()\n",
    "# Purpose: Assign unique sequential numbers within each department\n",
    "print(\"\\n--- 5. ROW_NUMBER() - Unique Ranking ---\")\n",
    "employees_ranked = employees_df.withColumn(\n",
    "    \"row_number\", row_number().over(department_window)\n",
    ").withColumn(\n",
    "    \"salary_rank\", rank().over(department_window)\n",
    ").withColumn(\n",
    "    \"dense_rank\", dense_rank().over(department_window)\n",
    ")\n",
    "\n",
    "print(\"Employees with Ranking Functions:\")\n",
    "employees_ranked.select(\n",
    "    \"employee_name\", \"department\", \"salary\", \n",
    "    \"row_number\",\"salary_rank\", \"dense_rank\"\n",
    ").orderBy(\"department\", col(\"salary\").desc()).show()\n",
    "\n",
    "\n",
    "# Operation 6: NTILE - Quartiles\n",
    "# Purpose: Divide employees into salary quartiles within departments\n",
    "print(\"\\n--- 6. NTILE() - Salary Quartiles ---\")\n",
    "employees_with_quartiles = employees_df.withColumn(\n",
    "    \"salary_quartile\", ntile(4).over(department_window)\n",
    ")\n",
    "\n",
    "print(\"Employees with Salary Quartiles:\")\n",
    "employees_with_quartiles.select(\n",
    "    \"employee_name\", \"department\", \"salary\", \"salary_quartile\"\n",
    ").orderBy(\"department\", \"salary_quartile\").show()\n",
    "\n",
    "# SECTION 5: WINDOW FUNCTIONS - ANALYTICAL\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 5: WINDOW FUNCTIONS - ANALYTICAL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Window for analytical functions (needs full window frame)\n",
    "department_window_analytical = Window.partitionBy(\"department\").orderBy(\"salary\").rowsBetween(\n",
    "    Window.unboundedPreceding, Window.unboundedFollowing\n",
    ")\n",
    "\n",
    "# Operation 7: LAG and LEAD\n",
    "# Purpose: Compare with previous and next values\n",
    "print(\"\\n--- 7. LAG() and LEAD() - Compare Adjacent Values ---\")\n",
    "salary_comparision = employees_df.withColumn(\n",
    "    \"previous_salary\", lag(\"salary\", 1).over(Window.partitionBy(\"department\").orderBy(\"salary\"))\n",
    ").withColumn(\n",
    "    \"next_salary\", lead(\"salary\", 1).over(Window.partitionBy(\"department\").orderBy(\"salary\"))\n",
    ").withColumn(\n",
    "    \"salary_growth\",\n",
    "    when(col(\"previous_salary\").isNull(), 0)\n",
    "    .otherwise(col(\"salary\") - col(\"previous_salary\"))\n",
    ").withColumn(\n",
    "    \"salary_decline\",\n",
    "    when(col(\"next_salary\").isNull(), 0)\n",
    "    .otherwise(col(\"next_salary\") - col(\"salary\"))\n",
    ")\n",
    "\n",
    "print(\"Salary Comparision with Previous/Next:\")\n",
    "salary_comparision.select(\n",
    "    \"employee_name\", \"department\", \"salary\", \"previous_salary\", \"next_salary\", \"salary_growth\", \"salary_decline\"\n",
    ").show()\n",
    "\n",
    "# Operation 8: FIRST_VALUE and LAST_VALUE\n",
    "# Purpose: Get extreme values in window\n",
    "print(\"\\n--- 8. FIRST_VALUE() and LAST_VALUE() ---\")\n",
    "salary_extremes = employees_df.withColumn(\n",
    "    \"department_min\", first(\"salary\").over(department_window_analytical)\n",
    ").withColumn(\n",
    "    \"department_max\", last(\"salary\").over(department_window_analytical)\n",
    ").withColumn(\n",
    "    \"diff_from_min\", col(\"salary\") - col(\"department_min\")\n",
    ").withColumn(\n",
    "    \"percent_of_max\", (col(\"salary\") / col(\"department_max\")) * 100\n",
    ")\n",
    "\n",
    "print(\"Salary Extremes within Departments:\")\n",
    "salary_extremes.select(\n",
    "    \"employee_name\", \"department\", \"salary\", \"department_min\", \"department_max\", \"diff_from_min\", \"percent_of_max\"\n",
    ").show()\n",
    "\n",
    "# SECTION 6: WINDOW FUNCTIONS - AGGREGATES\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 6: WINDOW FUNCTIONS - AGGREGATES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Operation 9: Running Totals and Moving Averages\n",
    "# Purpose: Calculate cumulative and moving aggregates\n",
    "print(\"\\n--- 9. Running Totals and Moving Averages ---\")\n",
    "\n",
    "# Define window for running calculations\n",
    "sales_window = Window.partitionBy(\"employee_id\").orderBy(\"sale_date\").rowsBetween(\n",
    "    Window.unboundedPreceding, Window.currentRow\n",
    ")\n",
    "\n",
    "# Define window for moving average (3 rows)\n",
    "moving_avg_window = Window.partitionBy(\"employee_id\").orderBy(\"sale_date\").rowsBetween(\n",
    "    -2, Window.currentRow\n",
    ")\n",
    "\n",
    "sales_analysis = employee_sales.withColumn(\n",
    "    \"running_total\", sum(\"amount\").over(sales_window)\n",
    ").withColumn(\n",
    "    \"moving_avg_3\", avg(\"amount\").over(moving_avg_window)\n",
    ").withColumn(\n",
    "    \"total_employee_sales\", sum(\"amount\").over(Window.partitionBy(\"employee_name\"))\n",
    ").withColumn(\n",
    "    \"percent_of_total\", (col(\"amount\") / col(\"total_employee_sales\")) * 100\n",
    ")\n",
    "\n",
    "print(\"Sales Analysis with Running Totals:\")\n",
    "sales_analysis.select(\n",
    "    \"employee_name\", \"sale_date\", \"amount\",\n",
    "    \"running_total\", \"moving_avg_3\",\n",
    "    \"percent_of_total\"\n",
    ").orderBy(\"employee_name\", \"sale_date\").show()\n",
    "\n",
    "\n",
    "# SECTION 7: COMPLEX WINDOW SCENARIOS\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 7: COMPLEX WINDOW SCENARIOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Operation 10: Multiple Window Functions Combined\n",
    "# Purpose: Comprehensive analysis using multiple windows\n",
    "print(\"\\n--- 10. Comprehensive Salary Analysis ---\")\n",
    "\n",
    "# Define multiple window specifications\n",
    "dept_window_rank = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "dept_window_agg = Window.partitionBy(\"department\")\n",
    "\n",
    "comprehensive_analysis = employees_df.withColumn(\n",
    "    \"dept_rank\", rank().over(dept_window_rank)\n",
    ").withColumn(\n",
    "    \"dept_avg_salary\", avg(\"salary\").over(dept_window_agg)\n",
    ").withColumn(\n",
    "    \"dept_max_salary\", max(\"salary\").over(dept_window_agg)\n",
    ").withColumn(\n",
    "    \"dept_min_salary\", min(\"salary\").over(dept_window_agg)\n",
    ").withColumn(\n",
    "    \"diff_from_avg\", col(\"salary\") - col(\"dept_avg_salary\")\n",
    ").withColumn(\n",
    "    \"percent_of_max\", (col(\"salary\") / col(\"dept_max_salary\")) * 100\n",
    ").withColumn(\n",
    "    \"salary_ratio\", col(\"salary\") / col(\"dept_avg_salary\")\n",
    ")\n",
    "\n",
    "print(\"Comprehensive Salary Analysis:\")\n",
    "comprehensive_analysis.select(\n",
    "    \"employee_name\", \"department\", \"salary\", \"dept_rank\",\n",
    "    \"dept_avg_salary\", \"diff_from_avg\", \"percent_of_max\", \"salary_ratio\"\n",
    ").orderBy(\"department\", \"dept_rank\").show()\n",
    "\n",
    "# Operation 11: Sales Performance with Multiple Dimensions\n",
    "print(\"\\n--- 11. Multi-dimensional Sales Analysis ---\")\n",
    "\n",
    "# Define multiple windows for different dimensions\n",
    "region_window = Window.partitionBy(\"region\").orderBy(col(\"amount\").desc())\n",
    "category_window = Window.partitionBy(\"product_category\").orderBy(col(\"amount\").desc())\n",
    "employee_region_window = Window.partitionBy(\"employee_name\", \"region\")\n",
    "\n",
    "sales_performance = employee_sales.withColumn(\n",
    "    \"regional_rank\", rank().over(region_window)\n",
    ").withColumn(\n",
    "    \"category_rank\", rank().over(category_window)\n",
    ").withColumn(\n",
    "    \"employee_region_avg\", avg(\"amount\").over(employee_region_window)\n",
    ").withColumn(\n",
    "    \"regional_avg\", avg(\"amount\").over(Window.partitionBy(\"region\"))\n",
    ").withColumn(\n",
    "    \"performance_vs_region\", col(\"amount\") - col(\"regional_avg\")\n",
    ")\n",
    "\n",
    "print(\"Sales Performance Analysis:\")\n",
    "sales_performance.select(\n",
    "    \"employee_name\", \"region\", \"product_category\", \"amount\",\n",
    "    \"regional_rank\", \"category_rank\", \n",
    "    \"employee_region_avg\", \"regional_avg\", \"performance_vs_region\"\n",
    ").orderBy(\"region\", \"regional_rank\").show()\n",
    "\n",
    "# SECTION 8: PRACTICAL BUSINESS SCENARIOS\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 8: PRACTICAL BUSINESS SCENARIOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Scenario 1: Employee Performance Bonus Calculation\n",
    "print(\"\\n--- Scenario 1: Performance Bonus Calculation ---\")\n",
    "\n",
    "bonus_calculation = comprehensive_analysis.withColumn(\n",
    "    \"bonus_tier\",\n",
    "    when(col(\"dept_rank\") == 1, \"Gold Bonus: 20%\")\n",
    "    .when(col(\"dept_rank\") == 2, \"Silver Bonus: 15%\")\n",
    "    .when(col(\"dept_rank\") == 3, \"Bronze Bonus: 10%\")\n",
    "    .otherwise(\"Standard Bonus: 5%\")\n",
    ").withColumn(\n",
    "    \"bonus_amount\",\n",
    "    when(col(\"dept_rank\") == 1, col(\"salary\") * 0.20)\n",
    "    .when(col(\"dept_rank\") == 2, col(\"salary\") * 0.15)\n",
    "    .when(col(\"dept_rank\") == 3, col(\"salary\") * 0.10)\n",
    "    .otherwise(col(\"salary\") * 0.05)\n",
    ")\n",
    "\n",
    "print(\"Employee Bonus Calculation:\")\n",
    "bonus_calculation.select(\n",
    "    \"employee_name\", \"department\", \"salary\", \"dept_rank\",\n",
    "    \"bonus_tier\", \"bonus_amount\"\n",
    ").orderBy(\"department\", \"dept_rank\").show()\n",
    "\n",
    "# Scenario 2: Sales Trend Analysis\n",
    "print(\"\\n--- Scenario 2: Sales Trend Analysis ---\")\n",
    "\n",
    "sales_trend_window = Window.partitionBy(\"employee_name\").orderBy(\"sale_date\")\n",
    "\n",
    "sales_trend_analysis = employee_sales.withColumn(\n",
    "    \"previous_sale\", lag(\"amount\", 1).over(sales_trend_window)\n",
    ").withColumn(\n",
    "    \"sales_trend\",\n",
    "    when(lag(\"amount\", 1).over(sales_trend_window).isNull(), \"First Sale\")\n",
    "    .when(col(\"amount\") > lag(\"amount\", 1).over(sales_trend_window), \"Growth\")\n",
    "    .when(col(\"amount\") < lag(\"amount\", 1).over(sales_trend_window), \"Decline\")\n",
    "    .otherwise(\"Stable\")\n",
    ").withColumn(\n",
    "    \"growth_percentage\",\n",
    "    when(lag(\"amount\", 1).over(sales_trend_window).isNull(), 0)\n",
    "    .otherwise(((col(\"amount\") - lag(\"amount\", 1).over(sales_trend_window)) / \n",
    "                lag(\"amount\", 1).over(sales_trend_window)) * 100)\n",
    ").filter(\n",
    "    col(\"sales_trend\").isNotNull()  # Remove rows where trend cannot be calculated\n",
    ")\n",
    "\n",
    "print(\"Sales Trend Analysis:\")\n",
    "sales_trend_analysis.select(\n",
    "    \"employee_name\", \"sale_date\", \"amount\", \"previous_sale\",\n",
    "    \"sales_trend\", round(\"growth_percentage\", 2).alias(\"growth_percentage\")\n",
    ").orderBy(\"employee_name\", \"sale_date\").show()\n",
    "\n",
    "# SECTION 9: DATAFRAME WRITING AND EXPORT\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 9: EXPORTING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save results to Delta tables (Databricks native format)\n",
    "print(\"\\n--- Saving Results to Delta Tables ---\")\n",
    "\n",
    "# Save employees with rankings\n",
    "employees_ranked.write.mode(\"overwrite\").saveAsTable(\"employee_rankings\")\n",
    "\n",
    "# Save sales analysis\n",
    "sales_analysis.write.mode(\"overwrite\").saveAsTable(\"sales_analysis\")\n",
    "\n",
    "# Save bonus calculation\n",
    "bonus_calculation.write.mode(\"overwrite\").saveAsTable(\"employee_bonuses\")\n",
    "\n",
    "print(\"Results saved to Delta tables:\")\n",
    "print(\"- employee_rankings\")\n",
    "print(\"- sales_analysis\") \n",
    "print(\"- employee_bonuses\")\n",
    "\n",
    "# Display saved tables\n",
    "print(\"\\n--- Displaying Saved Tables ---\")\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "# SECTION 10: PERFORMANCE OPTIMIZATION TIPS\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 10: PERFORMANCE OPTIMIZATION TIPS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\"\"\n",
    "DataFrame API Performance Best Practices:\n",
    "\n",
    "1. **Use Column Operations**: Always prefer column operations over UDFs when possible\n",
    "2. **Partition Wisely**: Window functions perform better with appropriate partitioning\n",
    "3. **Avoid Too Many Partitions**: Too many small partitions can degrade performance\n",
    "4. **Use Built-in Functions**: Spark's built-in functions are optimized\n",
    "5. **Cache Frequently Used DataFrames**: Use df.cache() for DataFrames used multiple times\n",
    "6. **Select Only Needed Columns**: Reduce data movement by selecting necessary columns\n",
    "\n",
    "Example of caching:\n",
    "employees_df.cache()  # Cache for repeated use\n",
    "\"\"\")\n",
    "\n",
    "# Cache example\n",
    "employees_df.cache()\n",
    "print(f\"Employees DataFrame cached: {employees_df.is_cached}\")\n",
    "\n",
    "# Clean up cache (optional)\n",
    "# employees_df.unpersist()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*50)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Learn_PySpark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
